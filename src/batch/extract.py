# src/batch/extract.py
from pyspark.sql.types import *
from pyspark.sql.functions import *

def parse_log_line(line):
    """ë¡œê·¸ ë¼ì¸ íŒŒì‹±"""
    try:
        fields = line.split('|')
        if len(fields) >= 10:
            return (
                fields[0],  # timestamp_ms
                fields[1],  # user_id
                fields[2],  # session_id
                fields[3],  # event_type
                fields[4],  # user_agent
                fields[5],  # ip_address
                fields[8],  # datetime_string
                fields[9]   # json_data
            )
    except:
        return None

def extract_data_from_s3(spark, date, s3_path):
    """
    S3ì—ì„œ íŠ¹ì • ë‚ ì§œì˜ ì›ë³¸ ë¡œê·¸ ë°ì´í„° ì½ê¸°
    
    Args:
        spark: SparkSession
        date: ë‚ ì§œ (YYYY-MM-DD)
        s3_path: S3 ê¸°ë³¸ ê²½ë¡œ
        
    Returns:
        DataFrame: íŒŒì‹±ëœ ë¡œê·¸ ë°ì´í„°
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“¥ [EXTRACT] S3 ë°ì´í„° ì½ê¸° ì‹œì‘: {date}")
    print(f"{'='*60}\n")
    
    # S3 ê²½ë¡œ
    full_path = f"{s3_path}date={date}/*.gz"
    print(f"ê²½ë¡œ: {full_path}")
    
    # RDDë¡œ ì½ê¸°
    rdd = spark.sparkContext.textFile(full_path)
    total_count = rdd.count()
    print(f"ì›ë³¸ ë ˆì½”ë“œ ìˆ˜: {total_count:,}ê±´\n")
    
    # íŒŒì‹±
    print("ğŸ”„ ë¡œê·¸ íŒŒì‹± ì¤‘...")
    parsed_rdd = rdd.map(parse_log_line).filter(lambda x: x is not None)
    
    # ìŠ¤í‚¤ë§ˆ ì •ì˜
    schema = StructType([
        StructField("timestamp_ms", StringType()),
        StructField("user_id", StringType()),
        StructField("session_id", StringType()),
        StructField("event_type", StringType()),
        StructField("user_agent", StringType()),
        StructField("ip_address", StringType()),
        StructField("datetime_string", StringType()),
        StructField("json_data", StringType())
    ])
    
    # DataFrame ìƒì„±
    df = spark.createDataFrame(parsed_rdd, schema)

    # JSON í•„ë“œ ì¶”ì¶œ (ì§€ì—­ ë¶„ì„ìš©)
    df = df.withColumn("local1", get_json_object(col("json_data"), "$.local1")) \
           .withColumn("local2", get_json_object(col("json_data"), "$.local2")) \
           .withColumn("room_type", get_json_object(col("json_data"), "$.room_type")) \
           .withColumn("deal_type", get_json_object(col("json_data"), "$.deal_type")) \
           .withColumn("screen_name", get_json_object(col("json_data"), "$.screen_name"))

    # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ (ë°€ë¦¬ì´ˆ í¬í•¨ ì²˜ë¦¬)
    df = df.withColumn(
        "timestamp",
        to_timestamp(col("datetime_string"), "yyyy-MM-dd HH:mm:ss.SSS")
    )

    # timestampê°€ nullì¸ ê²½ìš° ë°€ë¦¬ì´ˆ ì—†ëŠ” í˜•ì‹ìœ¼ë¡œ ì¬ì‹œë„
    df = df.withColumn(
        "timestamp",
        when(col("timestamp").isNull(),
             to_timestamp(col("datetime_string"), "yyyy-MM-dd HH:mm:ss")
        ).otherwise(col("timestamp"))
    ).withColumn(
        "date", lit(date)
    ).withColumn(
        "hour", hour(col("timestamp"))
    ).withColumn(
        "day_of_week", dayofweek(col("timestamp"))
    )
    
    parsed_count = df.count()
    print(f"íŒŒì‹± ì„±ê³µ: {parsed_count:,}ê±´")
    print(f"íŒŒì‹± ì‹¤íŒ¨: {total_count - parsed_count:,}ê±´")
    
    print(f"\nâœ… [EXTRACT] ì™„ë£Œ\n")
    
    return df

if __name__ == "__main__":
    import sys
    import os

    # Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ ìë™ ê°ì§€
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    sys.path.append(project_root)

    from src.config.config import Config

    # ì»¤ë§¨ë“œë¼ì¸ ì¸ìë¡œ ë‚ ì§œ ë°›ê¸°
    if len(sys.argv) > 1:
        date = sys.argv[1]
    else:
        date = "2018-01-26"

    print(f"\nğŸ”§ [EXTRACT] ì‹¤í–‰ ë‚ ì§œ: {date}")
    print(f"ğŸ“ ë°ì´í„° ê²½ë¡œ: {Config.get_data_path()}\n")

    spark = Config.get_spark_session(f"extract_{date}")

    try:
        # Extract: ë°ì´í„° ì½ê¸°
        df = extract_data_from_s3(spark, date, Config.get_data_path())

        # Silver layerë¡œ ì €ì¥ (Parquet)
        silver_path = f"{Config.get_results_path()}silver/date={date}"
        print(f"\nğŸ’¾ Silver layer ì €ì¥ ì¤‘: {silver_path}")
        df.write.mode("overwrite").parquet(silver_path)
        print(f"âœ… Silver layer ì €ì¥ ì™„ë£Œ\n")

        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ Extract ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        spark.stop()