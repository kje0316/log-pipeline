from pyspark.sql import DataFrame
import psycopg2


def delete_date_data(table_name, jdbc_url, properties, date):
    """
    íŠ¹ì • ë‚ ì§œì˜ ë°ì´í„°ë¥¼ PostgreSQL í…Œì´ë¸”ì—ì„œ ì‚­ì œ

    Args:
        table_name: í…Œì´ë¸”ëª…
        jdbc_url: JDBC URL (ì˜ˆ: jdbc:postgresql://localhost:5432/analytics)
        properties: JDBC ì†ì„±
        date: ì‚­ì œí•  ë‚ ì§œ (YYYY-MM-DD)
    """
    # JDBC URLì—ì„œ PostgreSQL ì—°ê²° ì •ë³´ ì¶”ì¶œ
    # jdbc:postgresql://localhost:5432/analytics -> localhost:5432/analytics
    url_parts = jdbc_url.replace("jdbc:postgresql://", "").split("/")
    host_port = url_parts[0].split(":")
    host = host_port[0]
    port = int(host_port[1]) if len(host_port) > 1 else 5432
    database = url_parts[1]

    # PostgreSQL ì—°ê²°
    conn = psycopg2.connect(
        host=host,
        port=port,
        database=database,
        user=properties["user"],
        password=properties["password"]
    )

    try:
        cursor = conn.cursor()

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables
                WHERE table_schema = 'public'
                AND table_name = %s
            );
        """, (table_name,))

        table_exists = cursor.fetchone()[0]

        if table_exists:
            delete_query = f"DELETE FROM {table_name} WHERE date = %s"
            cursor.execute(delete_query, (date,))
            deleted_count = cursor.rowcount
            conn.commit()

            if deleted_count > 0:
                print(f"   ğŸ—‘ï¸  ê¸°ì¡´ ë°ì´í„° ì‚­ì œ: {deleted_count:,}ê±´")
        else:
            print(f"   â„¹ï¸  í…Œì´ë¸” {table_name} ìƒì„± ì˜ˆì • (ì²« ì‹¤í–‰)")

    finally:
        cursor.close()
        conn.close()


def load_to_postgres(df, table_name, jdbc_url, properties, date=None, mode="append"):
    """
    DataFrameì„ PostgreSQLì— ì €ì¥

    Args:
        df: Spark DataFrame
        table_name: í…Œì´ë¸”ëª…
        jdbc_url: JDBC URL
        properties: JDBC ì†ì„±
        date: ë‚ ì§œ (ì¬ì²˜ë¦¬ë¥¼ ìœ„í•´ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ)
        mode: ì €ì¥ ëª¨ë“œ(append, overwrite)
    """
    print(f"\nğŸ’¾ [LOAD] {table_name} í…Œì´ë¸” ì €ì¥ ì¤‘...")

    # ì¬ì²˜ë¦¬ë¥¼ ìœ„í•´ ê¸°ì¡´ ë‚ ì§œ ë°ì´í„° ì‚­ì œ
    if date:
        delete_date_data(table_name, jdbc_url, properties, date)

    record_count = df.count()

    df.write.jdbc(jdbc_url, table_name, mode=mode, properties=properties)

    print(f"   âœ… {record_count:,}ê±´ ì €ì¥ ì™„ë£Œ")



def load_all_results(results, jdbc_url, properties, date=None):
    """
    ëª¨ë“  ë³€í™˜ ê²°ê³¼ë¥¼ PostgreSQLì— ì €ì¥ (ê¸°ì¡´ 5ê°œ + ì§€ì—­ ë¶„ì„ 7ê°œ + ê³ ë„í™” 4ê°œ = ì´ 16ê°œ)

    Args:
        results: ë³€í™˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        jdbc_url: JDBC URL
        properties: JDBC ì†ì„±
        date: ì²˜ë¦¬ ë‚ ì§œ (ì¬ì²˜ë¦¬ë¥¼ ìœ„í•´ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ)
    """
    print(f"\n{'='*60}")
    print("ğŸ’¾ [LOAD] PostgreSQL ì €ì¥ ì‹œì‘")
    print(f"{'='*60}")

    # ê¸°ì¡´ 5ê°œ í…Œì´ë¸” (date ì»¬ëŸ¼ í¬í•¨)
    load_to_postgres(results['hourly_stats'], 'hourly_stats', jdbc_url, properties, date)
    load_to_postgres(results['user_stats'], 'user_stats', jdbc_url, properties, date)
    load_to_postgres(results['event_stats'], 'event_stats', jdbc_url, properties, date)
    load_to_postgres(results['top_users'], 'top_users', jdbc_url, properties, date)
    load_to_postgres(results['daily_summary'], 'daily_summary', jdbc_url, properties, date)

    # ì§€ì—­ ë¶„ì„ 7ê°œ í…Œì´ë¸” (date ì»¬ëŸ¼ í¬í•¨ - ì¼ë³„ ì§‘ê³„)
    load_to_postgres(results['regional_search'], 'regional_search', jdbc_url, properties, date)
    load_to_postgres(results['room_preference'], 'room_preference', jdbc_url, properties, date)
    load_to_postgres(results['hourly_activity'], 'hourly_activity', jdbc_url, properties, date)
    load_to_postgres(results['engagement_index'], 'engagement_index', jdbc_url, properties, date)
    load_to_postgres(results['daily_activity_peak'], 'daily_activity_peak', jdbc_url, properties, date)
    load_to_postgres(results['hourly_deal'], 'hourly_deal', jdbc_url, properties, date)
    load_to_postgres(results['hourly_engagement'], 'hourly_engagement', jdbc_url, properties, date)

    # ê³ ë„í™” ë¶„ì„ 4ê°œ í…Œì´ë¸” (date ì»¬ëŸ¼ í¬í•¨ - ì¼ë³„ ì§‘ê³„)
    load_to_postgres(results['session_analysis'], 'session_analysis', jdbc_url, properties, date)
    load_to_postgres(results['user_behavior_patterns'], 'user_behavior_patterns', jdbc_url, properties, date)
    load_to_postgres(results['hourly_behavior_analysis'], 'hourly_behavior_analysis', jdbc_url, properties, date)
    load_to_postgres(results['funnel_analysis'], 'funnel_analysis', jdbc_url, properties, date)

    print(f"\nâœ… [LOAD] ì™„ë£Œ (ì´ 16ê°œ í…Œì´ë¸”)\n")



if __name__ == "__main__":
    import sys
    import os

    # Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ ìë™ ê°ì§€
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    sys.path.append(project_root)

    from src.config.config import Config

    # ì»¤ë§¨ë“œë¼ì¸ ì¸ìë¡œ ë‚ ì§œ ë°›ê¸°
    if len(sys.argv) > 1:
        date = sys.argv[1]
    else:
        date = "2018-01-26"

    print(f"\nğŸ”§ [LOAD] ì‹¤í–‰ ë‚ ì§œ: {date}\n")

    spark = Config.get_spark_session(f"load_{date}")

    try:
        # Gold layerì—ì„œ ë°ì´í„° ì½ê¸°
        gold_base_path = f"{Config.get_results_path()}gold/date={date}"
        print(f"ğŸ“¥ Gold layer ì½ê¸°: {gold_base_path}\n")

        results = {}
        table_names = [
            # ê¸°ì¡´ 5ê°œ
            'hourly_stats', 'user_stats', 'event_stats', 'top_users', 'daily_summary',
            # ì§€ì—­ ë¶„ì„ 7ê°œ
            'regional_search', 'room_preference', 'hourly_activity', 'engagement_index',
            'daily_activity_peak', 'hourly_deal', 'hourly_engagement',
            # ê³ ë„í™” ë¶„ì„ 4ê°œ
            'session_analysis', 'user_behavior_patterns', 'hourly_behavior_analysis', 'funnel_analysis'
        ]

        for table_name in table_names:
            gold_path = f"{gold_base_path}/{table_name}"
            results[table_name] = spark.read.parquet(gold_path)
            print(f"   âœ… {table_name} ë¡œë“œ ì™„ë£Œ: {results[table_name].count():,}ê±´")

        # JDBC ì†ì„±
        jdbc_properties = {
            "user": Config.POSTGRES_USER,
            "password": Config.POSTGRES_PASSWORD,
            "driver": "org.postgresql.Driver"
        }

        # PostgreSQLì— ì €ì¥
        load_all_results(results, Config.POSTGRES_JDBC_URL, jdbc_properties, date)

        print(f"\nâœ… [LOAD] ì™„ë£Œ! PostgreSQLì— ë°ì´í„° ì €ì¥ë¨\n")

        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ Load ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        spark.stop()