from pyspark.sql.functions import *
from pyspark.sql.window import Window

def apply_data_cleaning(df):
    """ë°ì´í„° í´ë¦¬ë‹: ì§€ì—­ëª… í‘œì¤€í™” ë° ì‹œê°„ íŒŒìƒ ì»¬ëŸ¼ ìƒì„±"""
    print("\nğŸ§¹ [CLEAN] ë°ì´í„° í´ë¦¬ë‹ ì‹œì‘...")

    # ì§€ì—­ëª… í‘œì¤€í™” ë§¤í•‘
    local1_mapping = {
        "ì„œìš¸": "ì„œìš¸íŠ¹ë³„ì‹œ", "ì„œìš¸ì‹œ": "ì„œìš¸íŠ¹ë³„ì‹œ", "ì„œìš¸íŠ¹ë³„ì‹œ": "ì„œìš¸íŠ¹ë³„ì‹œ",
        "ë¶€ì‚°": "ë¶€ì‚°ê´‘ì—­ì‹œ", "ë¶€ì‚°ì‹œ": "ë¶€ì‚°ê´‘ì—­ì‹œ", "ë¶€ì‚°ê´‘ì—­ì‹œ": "ë¶€ì‚°ê´‘ì—­ì‹œ",
        "ëŒ€êµ¬": "ëŒ€êµ¬ê´‘ì—­ì‹œ", "ëŒ€êµ¬ì‹œ": "ëŒ€êµ¬ê´‘ì—­ì‹œ", "ëŒ€êµ¬ê´‘ì—­ì‹œ": "ëŒ€êµ¬ê´‘ì—­ì‹œ",
        "ì¸ì²œ": "ì¸ì²œê´‘ì—­ì‹œ", "ì¸ì²œì‹œ": "ì¸ì²œê´‘ì—­ì‹œ", "ì¸ì²œê´‘ì—­ì‹œ": "ì¸ì²œê´‘ì—­ì‹œ",
        "ê´‘ì£¼": "ê´‘ì£¼ê´‘ì—­ì‹œ", "ê´‘ì£¼ì‹œ": "ê´‘ì£¼ê´‘ì—­ì‹œ", "ê´‘ì£¼ê´‘ì—­ì‹œ": "ê´‘ì£¼ê´‘ì—­ì‹œ",
        "ëŒ€ì „": "ëŒ€ì „ê´‘ì—­ì‹œ", "ëŒ€ì „ì‹œ": "ëŒ€ì „ê´‘ì—­ì‹œ", "ëŒ€ì „ê´‘ì—­ì‹œ": "ëŒ€ì „ê´‘ì—­ì‹œ",
        "ìš¸ì‚°": "ìš¸ì‚°ê´‘ì—­ì‹œ", "ìš¸ì‚°ì‹œ": "ìš¸ì‚°ê´‘ì—­ì‹œ", "ìš¸ì‚°ê´‘ì—­ì‹œ": "ìš¸ì‚°ê´‘ì—­ì‹œ",
        "ì„¸ì¢…": "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ", "ì„¸ì¢…ì‹œ": "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ", "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ": "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ",
        "ê²½ê¸°": "ê²½ê¸°ë„", "ê²½ê¸°ë„": "ê²½ê¸°ë„",
        "ê°•ì›": "ê°•ì›ë„", "ê°•ì›ë„": "ê°•ì›ë„",
        "ì¶©ë¶": "ì¶©ì²­ë¶ë„", "ì¶©ì²­ë¶ë„": "ì¶©ì²­ë¶ë„",
        "ì¶©ë‚¨": "ì¶©ì²­ë‚¨ë„", "ì¶©ì²­ë‚¨ë„": "ì¶©ì²­ë‚¨ë„",
        "ì „ë¶": "ì „ë¼ë¶ë„", "ì „ë¼ë¶ë„": "ì „ë¼ë¶ë„",
        "ì „ë‚¨": "ì „ë¼ë‚¨ë„", "ì „ë¼ë‚¨ë„": "ì „ë¼ë‚¨ë„",
        "ê²½ë¶": "ê²½ìƒë¶ë„", "ê²½ìƒë¶ë„": "ê²½ìƒë¶ë„",
        "ê²½ë‚¨": "ê²½ìƒë‚¨ë„", "ê²½ìƒë‚¨ë„": "ê²½ìƒë‚¨ë„",
        "ì œì£¼": "ì œì£¼íŠ¹ë³„ìì¹˜ë„", "ì œì£¼ë„": "ì œì£¼íŠ¹ë³„ìì¹˜ë„", "ì œì£¼íŠ¹ë³„ìì¹˜ë„": "ì œì£¼íŠ¹ë³„ìì¹˜ë„"
    }

    # ì§€ì—­ëª… í‘œì¤€í™”
    result_col = when((col("local1") == "NULL") | (col("local1").isNull()), lit(None))
    for original, standardized in local1_mapping.items():
        result_col = result_col.when(trim(col("local1")) == original, lit(standardized))
    result_col = result_col.otherwise(trim(col("local1")))

    # ì‹œê°„ íŒŒìƒ ì»¬ëŸ¼ ìƒì„±
    df_cleaned = df \
        .withColumn("log_time_ts", to_timestamp(col("timestamp"), "yyyy-MM-dd HH:mm:ss")) \
        .withColumn("search_hour", hour(col("log_time_ts"))) \
        .withColumn("is_weekend", when(dayofweek(col("log_time_ts")).isin([1, 7]), lit("ì£¼ë§")).otherwise(lit("í‰ì¼"))) \
        .withColumn("local1_std", result_col) \
        .withColumn("local1_original", col("local1")) \
        .drop("local1") \
        .withColumnRenamed("local1_std", "local1") \
        .drop("log_time_ts")

    print("   âœ… ì§€ì—­ëª… í‘œì¤€í™” ë° ì‹œê°„ íŒŒìƒ ì»¬ëŸ¼ ìƒì„± ì™„ë£Œ")
    return df_cleaned

def transform_hourly_stats(df):
    """ì‹œê°„ëŒ€ë³„ í†µê³„ ì§‘ê³„"""
    print("\nğŸ“Š [TRANSFORM] ì‹œê°„ëŒ€ë³„ í†µê³„ ì§‘ê³„ ì¤‘...")
    
    hourly_stats = df.groupBy("date", "hour").agg(
        count("*").alias("total_events"),
        countDistinct("user_id").alias("active_users"),
        countDistinct("session_id").alias("active_sessions"),
        countDistinct("ip_address").alias("unique_ips")
    ).orderBy("hour").withColumn(
        "date", to_date(col("date"))
    )

    print(f"   ì§‘ê³„ ì™„ë£Œ: {hourly_stats.count()}ê°œ ì‹œê°„ëŒ€")

    return hourly_stats


def transform_user_stats(df):
    """ìœ ì €ë³„ í†µê³„ ì§‘ê³„"""
    print("\nğŸ‘¥ [TRANSFORM] ìœ ì €ë³„ í†µê³„ ì§‘ê³„ ì¤‘...")
    
    user_stats = df.groupBy("date", "user_id").agg(
        count("*").alias("total_actions"),
        countDistinct("session_id").alias("session_count"),
        countDistinct("event_type").alias("unique_event_types"),
        min("timestamp").alias("first_action"),
        max("timestamp").alias("last_action")
    ).withColumn(
        "session_duration_sec",
        unix_timestamp("last_action") - unix_timestamp("first_action")
    ).withColumn(
        "avg_actions_per_session",
        col("total_actions") / col("session_count")
    ).withColumn(
        "date", to_date(col("date"))
    )

    print(f"   ì§‘ê³„ ì™„ë£Œ: {user_stats.count():,}ëª… ìœ ì €")

    return user_stats


def transform_event_stats(df):
    """ì´ë²¤íŠ¸ íƒ€ì…ë³„ í†µê³„ ì§‘ê³„"""
    print("\nğŸ“± [TRANSFORM] ì´ë²¤íŠ¸ë³„ í†µê³„ ì§‘ê³„ ì¤‘...")
    
    event_stats = df.groupBy("date", "event_type").agg(
        count("*").alias("event_count"),
        countDistinct("user_id").alias("unique_users"),
        countDistinct("session_id").alias("unique_sessions")
    ).withColumn(
        "avg_events_per_user",
        col("event_count") / col("unique_users")
    ).orderBy(desc("event_count")).withColumn(
        "date", to_date(col("date"))
    )

    print(f"   ì§‘ê³„ ì™„ë£Œ: {event_stats.count()}ê°œ ì´ë²¤íŠ¸ íƒ€ì…")

    return event_stats

def transform_top_users(df, top_n=100):
    """ê°€ì¥ í™œë°œí•œ ìœ ì € Top N"""
    print(f"\nğŸ† [TRANSFORM] Top {top_n} í™œì„± ìœ ì € ì§‘ê³„ ì¤‘...")
    
    top_users = df.groupBy("date", "user_id").agg(
        count("*").alias("total_actions")
    ).withColumn(
        "rank",
        row_number().over(Window.partitionBy("date").orderBy(desc("total_actions")))
    ).filter(col("rank") <= top_n).withColumn(
        "date", to_date(col("date"))
    )

    print(f"   ì§‘ê³„ ì™„ë£Œ: Top {top_n} ìœ ì €")

    return top_users


def transform_daily_summary(hourly_stats, user_stats, event_stats):
    """ì¼ë³„ ìš”ì•½ í†µê³„"""
    print("\nğŸ“ˆ [TRANSFORM] ì¼ë³„ ìš”ì•½ í†µê³„ ìƒì„± ì¤‘...")

    # ì‹œê°„ëŒ€ë³„ í†µê³„ì—ì„œ ì¼ë³„ í•©ê³„
    daily = hourly_stats.groupBy("date").agg(
        sum("total_events").alias("daily_total_events"),
        sum("active_users").alias("daily_total_users"),
        sum("active_sessions").alias("daily_total_sessions"),
        avg("active_users").alias("avg_hourly_users")
    ).withColumn(
        "date", to_date(col("date"))
    )

    print("   ìƒì„± ì™„ë£Œ")

    return daily


# ========================================
# ì§€ì—­ ë¶„ì„ ê´€ë ¨ Transform Functions
# ========================================

def transform_regional_search(df):
    """1. ì§€ì—­ë³„ ê²€ìƒ‰ëŸ‰ ì§‘ê³„"""
    print("\nğŸ—ºï¸  [TRANSFORM] ì§€ì—­ë³„ ê²€ìƒ‰ëŸ‰ ì§‘ê³„ ì¤‘...")

    regional_search = df.groupBy("date", "local1", "local2").agg(
        count("*").alias("total_searches")
    ).orderBy(col("local1").desc(), col("total_searches").desc()).withColumn(
        "date", to_date(col("date"))
    )

    print(f"   ì§‘ê³„ ì™„ë£Œ: {regional_search.count()}ê°œ ì§€ì—­")
    return regional_search


def transform_room_preference(df):
    """2. ì§€ì—­ë³„ ë§¤ë¬¼ ìœ í˜• ì„ í˜¸ë„ ë¶„ì„"""
    print("\nğŸ  [TRANSFORM] ì§€ì—­ë³„ ë§¤ë¬¼ ìœ í˜• ì„ í˜¸ë„ ì§‘ê³„ ì¤‘...")

    # NULL ê°’ ì œê±°
    df_filtered = df.filter(
        (col("local1").isNotNull()) &
        (col("local2").isNotNull()) &
        (col("room_type").isNotNull())
    )

    # ì§€ì—­ + ë§¤ë¬¼ ìœ í˜•ë³„ ê²€ìƒ‰ ìˆ˜ ì§‘ê³„
    df_room_counts = df_filtered.groupBy("date", "local1", "local2", "room_type").agg(
        count("*").alias("type_searches")
    )

    # ì§€ì—­ë³„ ì´ ê²€ìƒ‰ ìˆ˜ ì§‘ê³„
    df_total = df_room_counts.groupBy("date", "local1", "local2").agg(
        sum("type_searches").alias("total_regional_searches")
    )

    # ì¡°ì¸ ë° ì„ í˜¸ë„ ë¹„ìœ¨ ê³„ì‚°
    room_preference = df_room_counts.join(df_total, on=["date", "local1", "local2"], how="inner") \
        .withColumn("preference_rate", (col("type_searches") * 100.0 / col("total_regional_searches"))) \
        .orderBy("local1", "local2", col("preference_rate").desc()).withColumn(
            "date", to_date(col("date"))
        )

    print(f"   ì§‘ê³„ ì™„ë£Œ: {room_preference.count()}ê°œ ì§€ì—­-ìœ í˜• ì¡°í•©")
    return room_preference


def transform_hourly_activity(df):
    """3. ì§€ì—­ë³„ ì‹œê°„ëŒ€ë³„ í™œë™ëŸ‰ ì§‘ê³„"""
    print("\nâ° [TRANSFORM] ì§€ì—­ë³„ ì‹œê°„ëŒ€ë³„ í™œë™ëŸ‰ ì§‘ê³„ ì¤‘...")

    # NULL ê°’ ì œê±°
    df_filtered = df.filter(
        (col("local1").isNotNull()) &
        (col("local2").isNotNull()) &
        (col("search_hour").isNotNull()) &
        (col("is_weekend").isNotNull())
    )

    # ì§‘ê³„
    df_grouped = df_filtered.groupBy("date", "local1", "local2", "is_weekend", "search_hour").agg(
        count("*").alias("hourly_activity_count")
    )

    # í™œë™ëŸ‰ì´ ì ì€ ê·¸ë£¹ ì œê±° (count > 100)
    hourly_activity = df_grouped.filter(col("hourly_activity_count") > 100) \
        .orderBy("local1", "local2", "is_weekend", col("hourly_activity_count").desc()).withColumn(
            "date", to_date(col("date"))
        )

    print(f"   ì§‘ê³„ ì™„ë£Œ: {hourly_activity.count()}ê°œ ì‹œê°„ëŒ€")
    return hourly_activity


def transform_engagement_index(df):
    """4. ì§€ì—­ë³„ ì‚¬ìš©ì ëª°ì…ë„ ì§€ìˆ˜"""
    print("\nğŸ’¡ [TRANSFORM] ì§€ì—­ë³„ ì‚¬ìš©ì ëª°ì…ë„ ì§‘ê³„ ì¤‘...")

    # ì„¸ì…˜ë³„ ì´ë²¤íŠ¸ ìˆ˜ ì§‘ê³„
    session_event_counts = df.filter(
        (col("local2").isNotNull()) & (col("user_id").isNotNull())
    ).groupBy("date", "local1", "local2", "user_id").agg(
        count("*").alias("event_count_per_session")
    )

    # ì§€ì—­ë³„ í†µê³„
    engagement_index = session_event_counts.groupBy("date", "local1", "local2").agg(
        count("user_id").alias("total_sessions"),
        avg("event_count_per_session").alias("avg_events_per_session"),
        (sum(when(col("event_count_per_session") < 5, 1).otherwise(0)) * 100.0 / count("user_id")).alias("low_engagement_rate")
    ).filter(col("total_sessions") > 500) \
      .orderBy(col("avg_events_per_session").desc()).withColumn(
          "date", to_date(col("date"))
      )

    print(f"   ì§‘ê³„ ì™„ë£Œ: {engagement_index.count()}ê°œ ì§€ì—­")
    return engagement_index


def transform_daily_activity_peak(df):
    """5. ì‹œê°„ëŒ€ë³„ ì „ì²´ í™œë™ ì§‘ê³„"""
    print("\nğŸ“… [TRANSFORM] ì‹œê°„ëŒ€ë³„ ì „ì²´ í™œë™ ì§‘ê³„ ì¤‘...")

    # ìœ íš¨í•œ ì‹œê°„ ë¡œê·¸ë§Œ í•„í„°
    valid_logs = df.filter(col("search_hour").isNotNull())

    # ì‹œê°„ëŒ€ë³„ ì§‘ê³„
    daily_activity_peak = valid_logs.groupBy("date", "search_hour").agg(
        count("*").alias("hourly_activity_count"),
        countDistinct("user_id").alias("unique_users"),
        countDistinct("local1").alias("unique_local1"),
        countDistinct("local2").alias("unique_local2")
    ).orderBy(col("hourly_activity_count").desc()).withColumn(
        "date", to_date(col("date"))
    )

    print(f"   ì§‘ê³„ ì™„ë£Œ: {daily_activity_peak.count()}ê°œ ì‹œê°„ëŒ€")
    return daily_activity_peak


def transform_hourly_deal(df):
    """6. ì‹œê°„ëŒ€ë³„ ê±°ë˜ ìœ í˜• ì„ í˜¸ë„ ë³€í™”"""
    print("\nğŸ’° [TRANSFORM] ì‹œê°„ëŒ€ë³„ ê±°ë˜ ìœ í˜• ì„ í˜¸ë„ ì§‘ê³„ ì¤‘...")

    # í•„í„°: ìœ íš¨í•œ ì‹œê°„, ê±°ë˜ ìœ í˜•, ê²€ìƒ‰ í™œë™ ë¡œê·¸ë§Œ
    filtered_logs = df.filter(
        (col("search_hour").isNotNull()) &
        (col("deal_type").isNotNull()) &
        (col("screen_name").like("%í•„í„°%"))
    )

    # ì‹œê°„ëŒ€ + ê±°ë˜ ìœ í˜•ë³„ ì§‘ê³„
    hourly_deal = filtered_logs.groupBy("date", "search_hour", "deal_type", "screen_name").agg(
        count("*").alias("searches_by_deal_type"),
        countDistinct("user_id").alias("unique_users")
    ).orderBy(col("search_hour").asc(), col("searches_by_deal_type").desc()).withColumn(
        "date", to_date(col("date"))
    )

    print(f"   ì§‘ê³„ ì™„ë£Œ: {hourly_deal.count()}ê°œ ì‹œê°„ëŒ€-ê±°ë˜ ìœ í˜• ì¡°í•©")
    return hourly_deal


def transform_hourly_engagement(df):
    """7. ì‹œê°„ëŒ€ë³„ ì‚¬ìš©ì ëª°ì…ë„ ë³€í™”"""
    print("\nâš¡ [TRANSFORM] ì‹œê°„ëŒ€ë³„ ì‚¬ìš©ì ëª°ì…ë„ ì§‘ê³„ ì¤‘...")

    # ì„¸ì…˜ë³„, ì‹œê°„ëŒ€ë³„ ì´ë²¤íŠ¸ ìˆ˜ ì§‘ê³„
    hourly_event_counts = df.filter(
        (col("search_hour").isNotNull()) & (col("user_id").isNotNull())
    ).groupBy("date", "search_hour", "user_id").agg(
        count("*").alias("event_count_per_session")
    )

    # ì‹œê°„ëŒ€ë³„ í†µê³„ ê³„ì‚°
    hourly_engagement = hourly_event_counts.groupBy("date", "search_hour").agg(
        count("user_id").alias("total_sessions"),
        avg("event_count_per_session").alias("avg_events_per_session")
    ).filter(col("total_sessions") > 10) \
      .orderBy(col("avg_events_per_session").desc()).withColumn(
          "date", to_date(col("date"))
      )

    print(f"   ì§‘ê³„ ì™„ë£Œ: {hourly_engagement.count()}ê°œ ì‹œê°„ëŒ€")
    return hourly_engagement


def transform_session_analysis(df):
    """ì„¸ì…˜ ë¶„ì„: ì„¸ì…˜ë‹¹ ì´ë²¤íŠ¸ ìˆ˜, ì²´ë¥˜ì‹œê°„, ë°”ìš´ìŠ¤ìœ¨"""
    print("\nğŸ“Š [TRANSFORM] ì„¸ì…˜ ë¶„ì„ ì§‘ê³„ ì¤‘...")

    # ì„¸ì…˜ë³„ ì§‘ê³„
    session_stats = df.groupBy("date", "session_id", "user_id").agg(
        count("*").alias("events_per_session"),
        countDistinct("event_type").alias("unique_events"),
        (unix_timestamp(max("timestamp")) - unix_timestamp(min("timestamp"))).alias("session_duration_sec"),
        min("timestamp").alias("session_start"),
        max("timestamp").alias("session_end")
    )

    # ë°”ìš´ìŠ¤ ì„¸ì…˜ ì‹ë³„ (1ê°œ ì´ë²¤íŠ¸ë§Œ ë°œìƒ)
    session_stats = session_stats.withColumn(
        "is_bounce",
        when(col("events_per_session") == 1, 1).otherwise(0)
    )

    # ì¼ë³„ ì„¸ì…˜ í†µê³„
    daily_session_stats = session_stats.groupBy("date").agg(
        count("session_id").alias("total_sessions"),
        avg("events_per_session").alias("avg_events_per_session"),
        avg("session_duration_sec").alias("avg_session_duration_sec"),
        (sum("is_bounce") / count("session_id") * 100).alias("bounce_rate_pct"),
        countDistinct("user_id").alias("unique_users")
    ).withColumn("date", to_date(col("date")))

    print(f"   ì§‘ê³„ ì™„ë£Œ: {daily_session_stats.count()}ì¼")
    return daily_session_stats


def transform_user_behavior_patterns(df):
    """ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ ë¶„ì„: íŒŒì›Œìœ ì €, ê´€ì‹¬ ì§€ì—­, ë§¤ë¬¼ ì„ í˜¸ë„"""
    print("\nğŸ‘¤ [TRANSFORM] ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ ë¶„ì„ ì¤‘...")

    # ì‚¬ìš©ìë³„ í™œë™ ì§‘ê³„
    user_behavior = df.groupBy("date", "user_id").agg(
        count("*").alias("total_events"),
        countDistinct("session_id").alias("session_count"),
        countDistinct("event_type").alias("unique_event_types"),
        countDistinct("local1").alias("interested_regions"),
        collect_set("room_type").alias("room_types_viewed"),
        (countDistinct(when(col("event_type") == "view_item", col("event_type"))) / count("*") * 100).alias("view_item_rate")
    )

    # ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¥˜
    user_behavior = user_behavior.withColumn(
        "user_segment",
        when(col("total_events") >= 50, "íŒŒì›Œìœ ì €")
        .when(col("total_events") >= 20, "í™œì„±ìœ ì €")
        .when(col("total_events") >= 5, "ì¼ë°˜ìœ ì €")
        .otherwise("ì‹ ê·œìœ ì €")
    )

    # ì¼ë³„ ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ë³„ í†µê³„
    segment_stats = user_behavior.groupBy("date", "user_segment").agg(
        count("user_id").alias("user_count"),
        avg("total_events").alias("avg_events"),
        avg("session_count").alias("avg_sessions"),
        avg("interested_regions").alias("avg_regions")
    ).withColumn("date", to_date(col("date")))

    print(f"   ì§‘ê³„ ì™„ë£Œ: {segment_stats.count()}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
    return segment_stats


def transform_hourly_behavior_analysis(df):
    """ì‹œê°„ëŒ€ë³„ í–‰ë™ ë¶„ì„: í”¼í¬ íƒ€ì„, ì‹œê°„ëŒ€ë³„ ì „í™˜ìœ¨"""
    print("\nâ° [TRANSFORM] ì‹œê°„ëŒ€ë³„ í–‰ë™ ë¶„ì„ ì¤‘...")

    # ì‹œê°„ëŒ€ë³„ ì´ë²¤íŠ¸ ì§‘ê³„
    hourly_behavior = df.groupBy("date", "hour").agg(
        count("*").alias("total_events"),
        countDistinct("user_id").alias("active_users"),
        countDistinct("session_id").alias("total_sessions"),
        (count(when(col("event_type").isin(["view_search_results", "oneroom_filter_results", "apt_filter_results"]), 1)) / count("*") * 100).alias("search_rate"),
        (count(when(col("event_type") == "view_item", 1)) / count("*") * 100).alias("view_rate"),
        (count(when(col("event_type").isin(["add_to_wishlist", "add_to_cart"]), 1)) / count("*") * 100).alias("interest_rate")
    )

    # í”¼í¬ íƒ€ì„ ì—¬ë¶€ (í‰ê·  ëŒ€ë¹„ 120% ì´ìƒ)
    window_spec = Window.partitionBy("date")
    hourly_behavior = hourly_behavior.withColumn(
        "avg_events_per_hour", avg("total_events").over(window_spec)
    ).withColumn(
        "is_peak_time",
        when(col("total_events") >= col("avg_events_per_hour") * 1.2, 1).otherwise(0)
    ).drop("avg_events_per_hour") \
     .withColumn("date", to_date(col("date")))

    print(f"   ì§‘ê³„ ì™„ë£Œ: {hourly_behavior.count()}ê°œ ì‹œê°„ëŒ€")
    return hourly_behavior


def transform_funnel_analysis(df):
    """Funnel ë¶„ì„: ê²€ìƒ‰ â†’ ìƒì„¸ â†’ ê´€ì‹¬ â†’ êµ¬ë§¤ (4ë‹¨ê³„)"""
    print("\nğŸ” [TRANSFORM] Funnel ë¶„ì„ ì¤‘...")

    # Funnel ë‹¨ê³„ë³„ ì´ë²¤íŠ¸ ì •ì˜ (view_item_list ì œì™¸ - ì‹¤ì œ ë°ì´í„°ì—ì„œ ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
    funnel_events = df.withColumn(
        "funnel_stage",
        when(col("event_type").isin(["view_search_results", "oneroom_filter_results", "apt_filter_results"]), "1_search")
        .when(col("event_type") == "view_item", "2_detail")
        .when(col("event_type").isin(["add_to_wishlist", "add_to_cart"]), "3_interest")
        .when(col("event_type").isin(["begin_checkout", "purchase"]), "4_purchase")
        .otherwise(None)
    ).filter(col("funnel_stage").isNotNull())

    # ì„¸ì…˜ë³„ Funnel ë‹¨ê³„ ë„ë‹¬ ì—¬ë¶€
    session_funnel = funnel_events.groupBy("date", "session_id").agg(
        max(when(col("funnel_stage") == "1_search", 1).otherwise(0)).alias("reached_search"),
        max(when(col("funnel_stage") == "2_detail", 1).otherwise(0)).alias("reached_detail"),
        max(when(col("funnel_stage") == "3_interest", 1).otherwise(0)).alias("reached_interest"),
        max(when(col("funnel_stage") == "4_purchase", 1).otherwise(0)).alias("reached_purchase")
    )

    # ì¼ë³„ Funnel í†µê³„ (ìˆœì°¨ì  ì „í™˜ ì¶”ì )
    daily_funnel = session_funnel.groupBy("date").agg(
        count("session_id").alias("total_sessions"),
        sum("reached_search").alias("search_count"),
        sum("reached_detail").alias("detail_count"),
        sum("reached_interest").alias("interest_count"),
        sum("reached_purchase").alias("purchase_count"),
        # ìˆœì°¨ì  ì „í™˜ ì¹´ìš´íŠ¸
        sum(when((col("reached_search") == 1) & (col("reached_detail") == 1), 1).otherwise(0)).alias("search_and_detail"),
        sum(when((col("reached_detail") == 1) & (col("reached_interest") == 1), 1).otherwise(0)).alias("detail_and_interest"),
        sum(when((col("reached_interest") == 1) & (col("reached_purchase") == 1), 1).otherwise(0)).alias("interest_and_purchase"),
        sum(when((col("reached_search") == 1) & (col("reached_purchase") == 1), 1).otherwise(0)).alias("search_and_purchase")
    )

    # ì „í™˜ìœ¨ ê³„ì‚° (ìˆœì°¨ì  ì „í™˜ ê¸°ë°˜)
    daily_funnel = daily_funnel.withColumn(
        "search_to_detail_rate",
        when(col("search_count") > 0, (col("search_and_detail") / col("search_count") * 100)).otherwise(0)
    ).withColumn(
        "detail_to_interest_rate",
        when(col("detail_count") > 0, (col("detail_and_interest") / col("detail_count") * 100)).otherwise(0)
    ).withColumn(
        "interest_to_purchase_rate",
        when(col("interest_count") > 0, (col("interest_and_purchase") / col("interest_count") * 100)).otherwise(0)
    ).withColumn(
        "overall_conversion_rate",
        when(col("search_count") > 0, (col("search_and_purchase") / col("search_count") * 100)).otherwise(0)
    ).withColumn("date", to_date(col("date")))

    print(f"   ì§‘ê³„ ì™„ë£Œ: {daily_funnel.count()}ì¼")
    return daily_funnel


def run_all_transforms(df):
    """ëª¨ë“  ë³€í™˜ ì‹¤í–‰ (ê¸°ì¡´ 5ê°œ + ì§€ì—­ ë¶„ì„ 7ê°œ + ê³ ë„í™” 4ê°œ = ì´ 16ê°œ)"""
    print(f"\n{'='*60}")
    print("ğŸ”„ [TRANSFORM] ë°ì´í„° ë³€í™˜ ì‹œì‘")
    print(f"{'='*60}")

    # 1. ë°ì´í„° í´ë¦¬ë‹ (ì§€ì—­ëª… í‘œì¤€í™” ë° ì‹œê°„ íŒŒìƒ ì»¬ëŸ¼ ìƒì„±)
    df_cleaned = apply_data_cleaning(df)

    # 2. ê¸°ì¡´ ì§‘ê³„ ì‹¤í–‰ (5ê°œ)
    hourly_stats = transform_hourly_stats(df_cleaned)
    user_stats = transform_user_stats(df_cleaned)
    event_stats = transform_event_stats(df_cleaned)
    top_users = transform_top_users(df_cleaned, top_n=100)
    daily_summary = transform_daily_summary(hourly_stats, user_stats, event_stats)

    # 3. ì§€ì—­ ë¶„ì„ ì§‘ê³„ ì‹¤í–‰ (7ê°œ)
    regional_search = transform_regional_search(df_cleaned)
    room_preference = transform_room_preference(df_cleaned)
    hourly_activity = transform_hourly_activity(df_cleaned)
    engagement_index = transform_engagement_index(df_cleaned)
    daily_activity_peak = transform_daily_activity_peak(df_cleaned)
    hourly_deal = transform_hourly_deal(df_cleaned)
    hourly_engagement = transform_hourly_engagement(df_cleaned)

    # 4. ê³ ë„í™” ë¶„ì„ ì§‘ê³„ ì‹¤í–‰ (4ê°œ)
    session_analysis = transform_session_analysis(df_cleaned)
    user_behavior_patterns = transform_user_behavior_patterns(df_cleaned)
    hourly_behavior_analysis = transform_hourly_behavior_analysis(df_cleaned)
    funnel_analysis = transform_funnel_analysis(df_cleaned)

    print(f"\nâœ… [TRANSFORM] ì™„ë£Œ (ì´ 16ê°œ ì§‘ê³„)\n")

    return {
        # ê¸°ì¡´ 5ê°œ
        'hourly_stats': hourly_stats,
        'user_stats': user_stats,
        'event_stats': event_stats,
        'top_users': top_users,
        'daily_summary': daily_summary,
        # ì§€ì—­ ë¶„ì„ 7ê°œ
        'regional_search': regional_search,
        'room_preference': room_preference,
        'hourly_activity': hourly_activity,
        'engagement_index': engagement_index,
        'daily_activity_peak': daily_activity_peak,
        'hourly_deal': hourly_deal,
        'hourly_engagement': hourly_engagement,
        # ê³ ë„í™” ë¶„ì„ 4ê°œ
        'session_analysis': session_analysis,
        'user_behavior_patterns': user_behavior_patterns,
        'hourly_behavior_analysis': hourly_behavior_analysis,
        'funnel_analysis': funnel_analysis
    }

if __name__ == "__main__":
    import sys
    import os

    # Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ ìë™ ê°ì§€
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    sys.path.append(project_root)

    from src.config.config import Config

    # ì»¤ë§¨ë“œë¼ì¸ ì¸ìë¡œ ë‚ ì§œ ë°›ê¸°
    if len(sys.argv) > 1:
        date = sys.argv[1]
    else:
        date = "2018-01-26"

    print(f"\nğŸ”§ [TRANSFORM] ì‹¤í–‰ ë‚ ì§œ: {date}\n")

    spark = Config.get_spark_session(f"transform_{date}")

    try:
        # Silver layerì—ì„œ ë°ì´í„° ì½ê¸°
        silver_path = f"{Config.get_results_path()}silver/date={date}"
        print(f"ğŸ“¥ Silver layer ì½ê¸°: {silver_path}")
        df = spark.read.parquet(silver_path)
        print(f"âœ… Silver layer ë¡œë“œ ì™„ë£Œ: {df.count():,}ê±´\n")

        # Transform: ë°ì´í„° ë³€í™˜
        results = run_all_transforms(df)

        # Gold layerë¡œ ì €ì¥ (ê° í…Œì´ë¸”ë³„ë¡œ Parquet)
        gold_base_path = f"{Config.get_results_path()}gold/date={date}"
        print(f"\nğŸ’¾ Gold layer ì €ì¥ ì¤‘: {gold_base_path}")

        for table_name, df_result in results.items():
            gold_path = f"{gold_base_path}/{table_name}"
            df_result.write.mode("overwrite").parquet(gold_path)
            print(f"   âœ… {table_name} ì €ì¥ ì™„ë£Œ")

        print(f"\nâœ… Gold layer ì €ì¥ ì™„ë£Œ\n")

        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ Transform ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        spark.stop()
