import sys
import os

# Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ ìžë™ ê°ì§€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(project_root)

from src.config.config import Config
from src.batch.extract import extract_data_from_s3
from src.batch.transform import run_all_transforms
from src.batch.load import load_all_results

def run_etl_pipeline(date):
    """
    ì „ì²´ ETL íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    Args:
        date: ì²˜ë¦¬í•œ ë‚ ì§œ (YYYY-MM-DD)
    """
    
    print(f"\n{'='*70}")
    print(f"ðŸš€ ë°°ì¹˜ ETL íŒŒì´í”„ë¼ì¸ ì‹œìž‘: {date}")
    print(f"{'='*70}\n")
    
    # Spark ì„¸ì…˜ ìƒì„±
    spark = Config.get_spark_session(f"log_batch_etl_{date}")    
    
    try:
        # 1. Extract: ë°ì´í„° ì½ê¸°
        df = extract_data_from_s3(spark, date, Config.get_data_path())
        
        # 2. Transform: ë°ì´í„° ë³€í™˜/ì§‘ê³„ 
        results = run_all_transforms(df)
        
        # 3. Load: PostgreSQLì— ì €ìž¥
        jdbc_properties = {
            "user": Config.POSTGRES_USER,
            "password": Config.POSTGRES_PASSWORD,
            "driver": "org.postgresql.Driver"
        }
        load_all_results(results, Config.POSTGRES_JDBC_URL, jdbc_properties, date)
        
        print(f"\n{'='*70}")
        print(f"âœ… ë°°ì¹˜ ETL íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {date}")
        print(f"{'='*70}\n")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        spark.stop()



if __name__ == "__main__":
    # ì»¤ë§¨ë“œ ë¼ì¸ ì¸ìžë¡œ ë‚ ì§œ ë°›ê¸°
    if len(sys.argv) > 1:
        date = sys.argv[1]
    else:
        date = "2018-01-26"
    
    success = run_etl_pipeline(date)
    sys.exit(0 if success else 1)