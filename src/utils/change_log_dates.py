#!/usr/bin/env python3
"""
ë¡œê·¸ ë°ì´í„°ì˜ ë‚ ì§œë¥¼ 2018ë…„ì—ì„œ 2025ë…„ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import regexp_replace, col, expr
import os
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(project_root)

from src.config.config import Config

# ë‚ ì§œ ë§¤í•‘ (2018 â†’ 2025)
DATE_MAPPINGS = {
    "2018-01-26": "2025-10-26",
    "2018-01-27": "2025-10-27",
    "2018-01-28": "2025-10-28",
    "2018-01-29": "2025-10-29"
}

# ë°€ë¦¬ì´ˆ ì°¨ì´ ê³„ì‚° (2018-01-26 â†’ 2025-10-26 = 2838ì¼)
# 2838ì¼ * 24ì‹œê°„ * 3600ì´ˆ * 1000ë°€ë¦¬ì´ˆ = 245,203,200,000 ë°€ë¦¬ì´ˆ
MS_OFFSET_PER_DAY = {
    "2018-01-26": 245203200000,  # +2838ì¼
    "2018-01-27": 245203200000,  # +2838ì¼
    "2018-01-28": 245203200000,  # +2838ì¼
    "2018-01-29": 245203200000,  # +2838ì¼
}


def change_date_for_partition(spark, old_date, new_date, ms_offset):
    """
    íŠ¹ì • ë‚ ì§œ íŒŒí‹°ì…˜ì˜ ë°ì´í„°ë¥¼ ì½ì–´ì„œ ë‚ ì§œë¥¼ ë³€ê²½í•˜ê³  ì €ì¥

    Args:
        spark: SparkSession
        old_date: ê¸°ì¡´ ë‚ ì§œ (YYYY-MM-DD)
        new_date: ìƒˆ ë‚ ì§œ (YYYY-MM-DD)
        ms_offset: ë°€ë¦¬ì´ˆ ì˜¤í”„ì…‹
    """
    print(f"\n{'='*60}")
    print(f"ì²˜ë¦¬ ì¤‘: {old_date} â†’ {new_date}")
    print(f"{'='*60}\n")

    # ê¸°ì¡´ ë°ì´í„° ê²½ë¡œ
    old_path = f"{Config.LOCAL_RAW_PATH}date={old_date}"

    # CSV ì½ê¸° (íŒŒì´í”„ êµ¬ë¶„ì)
    print(f"ğŸ“¥ ì½ê¸°: {old_path}")
    df = spark.read.option("delimiter", "|").csv(old_path)

    original_count = df.count()
    print(f"   ì´ {original_count:,}ê±´")

    # ì»¬ëŸ¼ëª… ì§€ì •
    df = df.toDF(
        "timestamp_ms",      # 0
        "user_id",           # 1
        "session_id",        # 2
        "event_type",        # 3
        "user_agent",        # 4
        "ip_address",        # 5
        "hash_value",        # 6
        "timestamp_ms_2",    # 7
        "datetime_string",   # 8
        "json_data"          # 9
    )

    # ë‚ ì§œ ë³€ê²½
    print(f"ğŸ”„ ë‚ ì§œ ë³€ê²½ ì¤‘...")

    # 1. timestamp_ms ë³€ê²½ (ë°€ë¦¬ì´ˆ ë”í•˜ê¸°)
    df_changed = df.withColumn(
        "timestamp_ms",
        (col("timestamp_ms").cast("bigint") + ms_offset).cast("string")
    )

    # 2. timestamp_ms_2ë„ ë³€ê²½ (ê°™ì€ ë°©ì‹)
    df_changed = df_changed.withColumn(
        "timestamp_ms_2",
        (col("timestamp_ms_2").cast("bigint") + ms_offset).cast("string")
    )

    # 3. datetime_string ë³€ê²½ (ë¬¸ìì—´ ì¹˜í™˜)
    df_changed = df_changed.withColumn(
        "datetime_string",
        regexp_replace(col("datetime_string"), old_date, new_date)
    )

    # íŒŒì´í”„ êµ¬ë¶„ìë¡œ ê²°í•©
    df_output = df_changed.selectExpr(
        "concat_ws('|', " +
        "timestamp_ms, user_id, session_id, event_type, user_agent, " +
        "ip_address, hash_value, timestamp_ms_2, datetime_string, json_data) as value"
    )

    # ìƒˆ ê²½ë¡œì— ì €ì¥
    new_path = f"{Config.LOCAL_RAW_PATH}date={new_date}"
    print(f"ğŸ’¾ ì €ì¥: {new_path}")

    # CSV.GZë¡œ ì €ì¥ (ê¸°ì¡´ê³¼ ë™ì¼í•œ í¬ë§·)
    df_output.write.mode("overwrite").text(new_path, compression="gzip")

    # .txt.gz íŒŒì¼ì„ .csv.gzë¡œ rename (ë‚˜ì¤‘ì— ì²˜ë¦¬)
    print(f"   âœ… ì €ì¥ ì™„ë£Œ\n")

    return original_count


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("ë¡œê·¸ ë°ì´í„° ë‚ ì§œ ë³€ê²½ ìŠ¤í¬ë¦½íŠ¸ (2018 â†’ 2025)")
    print("="*60 + "\n")

    # Spark ì„¸ì…˜ ìƒì„±
    spark = SparkSession.builder \
        .appName("ChangeDates_2018_to_2025") \
        .master("local[*]") \
        .config("spark.driver.memory", "4g") \
        .getOrCreate()

    try:
        total_processed = 0

        # ê° ë‚ ì§œì— ëŒ€í•´ ì²˜ë¦¬
        for old_date, new_date in DATE_MAPPINGS.items():
            ms_offset = MS_OFFSET_PER_DAY[old_date]
            count = change_date_for_partition(spark, old_date, new_date, ms_offset)
            total_processed += count

        print("\n" + "="*60)
        print(f"âœ… ì™„ë£Œ! ì´ {total_processed:,}ê±´ ì²˜ë¦¬ë¨")
        print("="*60 + "\n")

        # íŒŒì¼ í™•ì¥ì ë³€ê²½ ì•ˆë‚´
        print("âš ï¸  ì°¸ê³ : ì €ì¥ëœ íŒŒì¼ì´ .txt.gzë¡œ ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("    í•„ìš”ì‹œ .csv.gzë¡œ rename í•´ì£¼ì„¸ìš”.\n")

    finally:
        spark.stop()


if __name__ == "__main__":
    main()
